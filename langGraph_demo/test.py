
import time
from openai import OpenAI
import sys
import os
import requests

# è®¾ç½®ç¯å¢ƒå˜é‡ï¼Œç¡®ä¿ä¸èµ°ä»£ç†
os.environ['HTTP_PROXY'] = ''
os.environ['HTTPS_PROXY'] = ''
os.environ['http_proxy'] = ''
os.environ['https_proxy'] = ''
os.environ['NO_PROXY'] = '*'

# å¤‡é€‰æ–¹æ¡ˆï¼šä½¿ç”¨requestsåº“è®¾ç½®ä»£ç†
# session = requests.Session()
# session.proxies = {
#     'http': None,
#     'https': None
# }

# Add the parent directory to the Python path so we can import config
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
# æµ‹è¯• prompt å†…å®¹
prompt = "ç”Ÿæˆä¸€å¼ æ³¢æ–¯çŒ«çš„å›¾ç‰‡"

from config import model_configs

# ç»Ÿä¸€è¾“å‡ºæ ¼å¼
def print_result(name, token_count, start_time, first_token_time, end_time):
    print(f"\nğŸ§ª æ¨¡å‹åç§°ï¼š{name}")
    if first_token_time:
        print(f"â± é¦– token å“åº”æ—¶é—´ï¼š{first_token_time - start_time:.2f} ç§’")
    print(f"â± æ€»è€—æ—¶ï¼š{end_time - start_time:.2f} ç§’")
    if first_token_time:
        duration = end_time - first_token_time
        print(f"ğŸ“ˆ å¹³å‡ååç‡ï¼š{token_count / duration:.2f} tokens/sec")
    print("-" * 40)

# ä½¿ç”¨å•ä¸ªæ¨¡å‹é…ç½®è¿›è¡Œæµ‹è¯•
print(f"\n========== æ­£åœ¨æµ‹è¯•æ¨¡å‹ï¼š{model_configs['model_name']} ==========")
client = OpenAI(
    base_url=model_configs["base_url"],
    api_key=model_configs["api_key"],
    http_client=None,  # ä¸ä½¿ç”¨ä»£ç†
)

start_time = time.time()
first_token_time = None
token_count = 0

response = client.chat.completions.create(
    messages=[{"role": "user", "content": prompt}],
    model=model_configs["model_name"],
    temperature=1.0,
    stream=True,
    timeout=60
)

for chunk in response:
    delta = chunk.choices[0].delta
    if delta.content:
        print(delta.content, end="", flush=True)
        token_count += 1
        if first_token_time is None:
            first_token_time = time.time()

end_time = time.time()
print_result(model_configs["model_name"], token_count, start_time, first_token_time, end_time)