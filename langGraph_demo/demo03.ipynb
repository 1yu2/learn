{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c7efea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# è®¾ç½®ç¯å¢ƒå˜é‡ï¼Œç¡®ä¿ä¸èµ°ä»£ç†\n",
    "os.environ['HTTP_PROXY'] = ''\n",
    "os.environ['HTTPS_PROXY'] = ''\n",
    "os.environ['http_proxy'] = ''\n",
    "os.environ['https_proxy'] = ''\n",
    "os.environ['NO_PROXY'] = '*'\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "model_configs = {\n",
    "    \"base_url\": \"http://192.168.10.60:11434/v1/\",\n",
    "    \"model_name\": 'gpt-oss:20b',\n",
    "    \"api_key\": \"289751f29159b7d25dadc86c858fa48405bba6740acb57154b00494c0308ed95\"\n",
    "}\n",
    "\n",
    "# åˆ›å»ºLangChainçš„ChatOpenAIå®¢æˆ·ç«¯ï¼ˆæ”¯æŒbind_toolsï¼‰\n",
    "llm = ChatOpenAI(\n",
    "    base_url=model_configs[\"base_url\"],\n",
    "    api_key=model_configs[\"api_key\"],\n",
    "    model=model_configs[\"model_name\"],\n",
    "    temperature=1.0,\n",
    "    timeout=180\n",
    ")\n",
    "\n",
    "# æµ‹è¯•åŸºæœ¬è°ƒç”¨\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"What is LangGraph?\"},\n",
    "]\n",
    "response = llm.invoke(\"What is LangGraph?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df5ae53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'openai.types.chat.chat_completion.ChatCompletion'>\n",
      "**LangGraph** is an openâ€‘source framework for building largeâ€‘languageâ€‘model (LLM) driven applications that run across multiple turns, modalities, and external toolsâ€”all orchestrated with an explicit graph of logic.\n",
      "\n",
      "---\n",
      "\n",
      "### Why do we need a â€œgraphâ€ for LLM apps?\n",
      "\n",
      "| Problem | Traditional approach | LangGraph advantage |\n",
      "|---------|----------------------|---------------------|\n",
      "| **Control flow** (which tool to call next, when to stop) | Hardâ€‘coded in the prompt or a flat chain | Explicit nodes & edges let you design branching, loops, backâ€‘tracing, and dynamic paths |\n",
      "| **State management** (memory, context, user intent) | Implicitly baked into the prompt | `state` is a firstâ€‘class dictionary that can be read/written by any node |\n",
      "| **Tool integration** (search, API calls, spreadsheet, etc.) | Often a generic â€œtoolâ€‘callingâ€ prompt that may fail | Dedicated nodes for APIs, DBs, HTTP, even custom Python functions |\n",
      "| **Debugging / monitoring** | You have to trace logs manually | Builtâ€‘in visualizer, event hooks, live tracing |\n",
      "| **Extensibility** | Each app needs its own plumbing | Graph definition is reusable; you can import/export graphs |\n",
      "\n",
      "---\n",
      "\n",
      "## Core Concepts\n",
      "\n",
      "| Concept | What it is | How it helps |\n",
      "|---------|------------|--------------|\n",
      "| **Graph** | A directed graph where each vertex is a *node* and edges define the allowed next steps. | Lets you program logic as flowcharts instead of linear code. |\n",
      "| **Node** | A Python callable (function, LLM chain, toolâ€¦) that receives the current `state` and emits a new `state`. | Encapsulates a unit of work (e.g. â€œask LLMâ€, â€œcall APIâ€, â€œsummarizeâ€). |\n",
      "| **State** | A dictionary that persists across nodes. Keys can be anything: text, JSON, scores, context vectors. | Gives you full control over what data travels through the graph. |\n",
      "| **Edge** | The connection from one node to another. Usually youâ€™ll annotate it with a *condition* (`if`, `else`, or predicates). | Directs the flow based on runtime data. |\n",
      "| **Memory** | External store (vector DB, relational DB, Redisâ€¦) that nodes can read/write. | Enables retrievalâ€‘augmented generation, conversation history, knowledge bases. |\n",
      "| **Event Hooks** | Callbacks fired before/after nodes run or on errors. | Logging, monitoring, A/B testing. |\n",
      "| **Visualizer** | Web UI that shows the graph, the current state, and the runtime path. | Debugging made painless. |\n",
      "| **Tool Nodes** | Nodes that wrap an LLM tool call (HTTP, Python function, SQL). | Clean separation; you can swap tools without changing the graph structure. |\n",
      "| **ReAct / Memoryâ€‘Augmented Patterns** | Standard patterns like â€œReActâ€ (Reason + Act), â€œToolâ€‘chainâ€ are available as preâ€‘built node templates. | Rapid prototyping. |\n",
      "\n",
      "---\n",
      "\n",
      "## How a LangGraph App Works\n",
      "\n",
      "1. **Define the Graph**  \n",
      "   ```python\n",
      "   from langgraph import Graph\n",
      "   g = Graph()\n",
      "   g.add_nodes({\n",
      "       \"read_user\": read_user_node,\n",
      "       \"extract_query\": query_node,\n",
      "       \"search_db\": search_node,\n",
      "       \"llm_answer\": answer_node,\n",
      "       \"finalize\": finalize_node,\n",
      "   })\n",
      "   g.set_start(\"read_user\")\n",
      "   g.set_end(\"finalize\")\n",
      "   # Add edges\n",
      "   g.add_edge(\"read_user\",\"extract_query\")\n",
      "   g.add_edge(\"extract_query\",\"search_db\")\n",
      "   g.add_edge(\"search_db\",\"llm_answer\")\n",
      "   g.add_edge(\"llm_answer\",\"finalize\")\n",
      "   ```\n",
      "2. **Run the Graph**  \n",
      "   ```python\n",
      "   result = g.run(initial_state={\"message\": \"Show me the latest quarterly earnings\"} )\n",
      "   ```\n",
      "3. **Graph handles**  \n",
      "   * Passes `state` along nodes automatically.  \n",
      "   * Evaluates any conditional edges.  \n",
      "   * Traces the path for debugging.  \n",
      "   * Stores intermediate results in memory if configured.\n",
      "\n",
      "---\n",
      "\n",
      "## When to Use LangGraph\n",
      "\n",
      "| Scenario | Example Useâ€‘case |\n",
      "|-----------|-----------------|\n",
      "| **Multiâ€‘turn dialogue** | Chatbot that keeps context, updates intent, and calls external APIs on demand. |\n",
      "| **Adaptive reasoning** | Decision support system that loops between inference steps until a confidence threshold is reached. |\n",
      "| **Toolâ€‘heavy pipelines** | A dataâ€‘augmentation pipeline that calls language models, then a QA tool, then stores results in a vector DB. |\n",
      "| **Explainable AI** | A graph that logs every reasoning step, so the final answer can be traced back to specific prompts or external data. |\n",
      "| **Rapid prototyping** | Swap LLM backends (OpenAI, Anthropic, local) without touching the graph. |\n",
      "\n",
      "---\n",
      "\n",
      "## Ecosystem & Integrations\n",
      "\n",
      "| Tool | Where it fits in LangGraph |\n",
      "|------|----------------------------|\n",
      "| **LangChain** | You can import any LangChain chain or prompt template as a node (`chain_node`). |\n",
      "| **LangSmith** | Builtâ€‘in monitoring & tracing (metrics, logs, vector embeddings). |\n",
      "| **LangGraph UI** | Live graph viewer, replay history, error inspection. |\n",
      "| **Tools**: `requests`, `python` functions, `SQL`, `Google Search`, `OpenAI`, `Anthropic`, `OpenRouter`, `Azure`, `OpenAI Embedding APIs`. |\n",
      "| **Memory**: FAISS, Pinecone, Weaviate, Chroma, Qdrant, Supabase. |\n",
      "| **Embeddings**: OpenAI, Cohere, HuggingFace, all accessible as node wrappers. |\n",
      "\n",
      "---\n",
      "\n",
      "## Example: Simple QA Graph\n",
      "\n",
      "```python\n",
      "from langgraph import Graph\n",
      "from langgraph.operators import ToolNode, LLMNode, State\n",
      "\n",
      "def search_tool(query: str) -> str:\n",
      "    # pretend we hit a vector DB\n",
      "    return \"Retrieved content about quarterly earnings.\"\n",
      "\n",
      "g = Graph()\n",
      "g.add_nodes({\n",
      "    \"extract_query\": LLMNode(\"extract_query_prompt\"),\n",
      "    \"search\": ToolNode(\"search\", function=search_tool),\n",
      "    \"respond\": LLMNode(\"answer_prompt\")\n",
      "})\n",
      "g.set_start(\"extract_query\")\n",
      "g.set_end(\"respond\")\n",
      "g.add_edge(\"extract_query\", \"search\")\n",
      "g.add_edge(\"search\", \"respond\")\n",
      "\n",
      "response = g.run({\"user_input\": \"What did the company earn this quarter?\"})\n",
      "print(response[\"final_answer\"])\n",
      "```\n",
      "\n",
      "Output (mocked):\n",
      "\n",
      "```\n",
      "The company earned $3.2B in the latest quarter, representing a 12% YoY increase.\n",
      "```\n",
      "\n",
      "---\n",
      "\n",
      "## Getting Started\n",
      "\n",
      "1. **Install**  \n",
      "   ```bash\n",
      "   pip install langgraph\n",
      "   ```\n",
      "2. **Read the docs** â€“ https://docs.langgraph.xyz  \n",
      "3. **Try the playground** â€“ Build a graph in a Jupyter notebook or Google Colab.  \n",
      "4. **Deploy** â€“ Export your graph as a serverless function, wrap with FastAPI, or use it in a custom UI.\n",
      "\n",
      "---\n",
      "\n",
      "### Takeaway\n",
      "\n",
      "LangGraph turns the blackâ€‘box chainâ€‘ofâ€‘thought model into a *graph* you can program, inspect, and evolve. Itâ€™s the glue that lets you:\n",
      "\n",
      "- Keep state safe and explicit,\n",
      "- Call any external tool cleanly,\n",
      "- Trace every decision step,\n",
      "- Swap LLMs without touching logic.\n",
      "\n",
      "If youâ€™re building an AI app that needs to reason over past context, talk to APIs, and deliver explainable answers, LangGraph gives you the right abstraction to avoid reinventing plumbing every time.\n"
     ]
    }
   ],
   "source": [
    "print(type(response))\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c717d33a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 'chatcmpl-464', 'object': 'chat.completion', 'created': 1754559472, 'model': 'gpt-oss:20b', 'system_fingerprint': 'fp_ollama', 'choices': [{'index': 0, 'message': {'role': 'assistant', 'content': '## LangGraph â€“ A Graphâ€‘Based Framework for LLMâ€‘Powered Apps\\n\\n**LangGraph** is an openâ€‘source framework (part of the LangChain ecosystem) that lets you build, orchestrate, and debug complex, stateful workflows powered by large language models (LLMs).  \\nInstead of writing a monolithic â€œagentâ€ or a linear script, you describe a *graph* of nodes (functions, prompts, tools, etc.) and the edges that connect them. The framework takes care of:\\n\\n| What it does | Why it matters |\\n|--------------|----------------|\\n| **Graph execution** | You can run nodes in sequence, in parallel, or conditionally based on the current state. |\\n| **Stateful reasoning** | Each node receives the full â€œstateâ€ (conversation history, memory, tool outputs, etc.) and can update it. |\\n| **Tool integration** | Builtâ€‘in connectors for APIs, databases, custom functions, and the LangChain toolâ€‘calling system. |\\n| **LLM orchestration** | Wrap any LLM (OpenAI, Anthropic, Gemini, local models, etc.) as a node. |\\n| **Memory & context** | Persist state across turns, enabling longâ€‘term memory or multiâ€‘step reasoning. |\\n| **Debugging & visualization** | LangGraph Studio lets you see the graph, step through execution, and inspect state at each node. |\\n| **Composable** | Reuse subâ€‘graphs (e.g., a â€œsearchâ€‘thenâ€‘summarizeâ€ subâ€‘graph) across multiple applications. |\\n\\n---\\n\\n### How It Looks in Code\\n\\n```python\\nfrom langgraph.graph import StateGraph, MessagesState\\nfrom langchain_openai import ChatOpenAI\\nfrom langchain.tools import DuckDuckGoSearchRun\\n\\n# 1. Define the LLM node\\nllm = ChatOpenAI(model=\"gpt-4o-mini\")\\n\\ndef ask_llm(state: MessagesState) -> MessagesState:\\n    # Pass the conversation history to the LLM\\n    response = llm.invoke(state[\"messages\"])\\n    state[\"messages\"].append(response)\\n    return state\\n\\n# 2. Define a tool node\\nsearch_tool = DuckDuckGoSearchRun()\\n\\ndef search(state: MessagesState) -> MessagesState:\\n    query = state[\"messages\"][-1][\"content\"]  # last user message\\n    result = search_tool.run(query)\\n    state[\"search_result\"] = result\\n    return state\\n\\n# 3. Build the graph\\ngraph = StateGraph(MessagesState)\\ngraph.add_node(\"ask_llm\", ask_llm)\\ngraph.add_node(\"search\", search)\\n\\n# Connect nodes: after asking the LLM, decide whether to search\\ngraph.add_conditional_edges(\\n    \"ask_llm\",\\n    lambda state: \"search\" if \"search\" in state[\"messages\"][-1][\"content\"].lower() else \"ask_llm\",\\n    {\"search\": \"search\", \"ask_llm\": \"ask_llm\"}\\n)\\n\\n# Final node that returns the answer\\ndef finish(state: MessagesState) -> MessagesState:\\n    # Combine LLM answer with search result if available\\n    if \"search_result\" in state:\\n        state[\"messages\"].append({\"role\": \"assistant\", \"content\": state[\"search_result\"]})\\n    return state\\n\\ngraph.add_node(\"finish\", finish)\\ngraph.set_entry_point(\"ask_llm\")\\ngraph.set_finish_point(\"finish\")\\n\\n# 4. Run the graph\\napp = graph.compile()\\nresponse = app.invoke({\"messages\": [{\"role\": \"user\", \"content\": \"Tell me about the Eiffel Tower\"}]})\\nprint(response[\"messages\"][-1][\"content\"])\\n```\\n\\n*What happens?*  \\n1. The user message is sent to the LLM.  \\n2. The LLMâ€™s reply is examined; if it contains the word â€œsearchâ€, the graph routes to the `search` node.  \\n3. The search result is stored in state and the graph returns to `finish`, which merges the search output with the LLM answer.  \\n\\n---\\n\\n### Key Concepts\\n\\n| Concept | Description |\\n|---------|-------------|\\n| **State** | A dictionary (or custom class) that carries all data through the graph. |\\n| **Node** | A callable that receives the state, performs work (LLM call, tool usage, logic), and returns an updated state. |\\n| **Edge** | Defines the flow between nodes. Can be static or conditional based on the state. |\\n| **Graph** | The collection of nodes and edges. You compile it into an executable app. |\\n| **LangGraph Studio** | A web UI that visualizes the graph, shows node execution order, and lets you step through runs. |\\n\\n---\\n\\n### Why Use LangGraph?\\n\\n| Scenario | LangGraph Advantage |\\n|----------|---------------------|\\n| **Multiâ€‘step reasoning** | Each step can be a separate node, making the logic clear and testable. |\\n| **Toolâ€‘calling** | Wrap any external API or function as a node; the graph handles when to call it. |\\n| **Longâ€‘term memory** | Persist state across turns; you can store embeddings, search results, or user preferences. |\\n| **Debugging** | Visualize the flow, inspect state at each node, and replay runs. |\\n| **Modularity** | Build reusable subâ€‘graphs (e.g., â€œsummarizeâ€, â€œtranslateâ€) and compose them into larger workflows. |\\n\\n---\\n\\n### Getting Started\\n\\n```bash\\npip install langgraph langchain-openai duckduckgo-search\\n```\\n\\n```python\\n# Quick demo\\nfrom langgraph.graph import StateGraph, MessagesState\\nfrom langchain_openai import ChatOpenAI\\n\\nllm = ChatOpenAI(model=\"gpt-4o-mini\")\\n\\ndef ask(state: MessagesState) -> MessagesState:\\n    response = llm.invoke(state[\"messages\"])\\n    state[\"messages\"].append(response)\\n    return state\\n\\ngraph = StateGraph(MessagesState)\\ngraph.add_node(\"ask\", ask)\\ngraph.set_entry_point(\"ask\")\\ngraph.set_finish_point(\"ask\")\\napp = graph.compile()\\n\\nprint(app.invoke({\"messages\": [{\"role\": \"user\", \"content\": \"Hello\"}]}}))\\n```\\n\\n---\\n\\n### Where to Learn More\\n\\n| Resource | What youâ€™ll find |\\n|----------|------------------|\\n| **GitHub repo** | `https://github.com/langchain-ai/langgraph` â€“ source code, docs, examples. |\\n| **Docs** | `https://langgraph.org/docs` â€“ tutorials, API reference, best practices. |\\n| **LangGraph Studio** | `https://studio.langgraph.org` â€“ visual debugging (requires a free account). |\\n| **Community** | LangChain Discord, StackOverflow tags, and the LangChain forum. |\\n\\n---\\n\\n### Bottom Line\\n\\nLangGraph turns the oftenâ€‘messy process of building LLMâ€‘driven applications into a **structured, visual, and testable** workflow. By treating each step as a node in a graph, you gain fineâ€‘grained control over when and how the LLM, tools, and memory interactâ€”making it easier to build reliable, maintainable, and explainable AI systems.', 'reasoning': 'The user asks: \"What is LangGraph?\" They likely refer to a library or framework. LangGraph is a framework for building LLM-powered applications, especially for building graph-based workflows. It\\'s by the LangChain team? Actually LangGraph is a framework by the LangChain team for building LLM-powered applications with graph-based logic. It allows building stateful, multi-step reasoning, and can be used for building agents, workflows, etc. It uses graph nodes, edges, and state. It can integrate with LLMs, memory, tools, etc. It is open-source. It is part of the LangChain ecosystem. It uses the concept of \"LangGraph\" to orchestrate LLM calls, memory, and tool usage. It can be used to build complex applications like chatbots, data pipelines, etc. It uses a graph of nodes that can be executed sequentially or conditionally. It can be used with LangChain\\'s LLMs, memory, and tools. It also supports \"LangGraph Studio\" for visualizing and debugging. It can be used with Python. It can be integrated with LangChain\\'s \"Agent\" concept. It can also be used with \"LangGraph\" to build \"LangGraph\" flows. It can be used to build \"LangGraph\" for \"LangGraph\" tasks. The answer should explain what LangGraph is, its purpose, key features, how it works, and maybe mention its relation to LangChain. Also mention that it\\'s open-source, available on GitHub, and can be installed via pip. Provide a brief example. Also mention that it\\'s used for building LLM-powered applications with graph-based logic, enabling multi-step reasoning, tool usage, memory, etc. Also mention that it can be used for building agents, workflows, and can be integrated with other frameworks. Provide a concise but thorough explanation. Also mention that it\\'s part of the LangChain ecosystem. Provide a short code snippet. Also mention that it uses \"state\" and \"nodes\" and \"edges\". Also mention that it can be used with \"LangGraph Studio\" for debugging. Also mention that it\\'s built on top of \"LangChain\" and \"LangChain Core\". Also mention that it\\'s used for building \"LangGraph\" for \"LangGraph\" tasks. Provide a final answer.'}, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 72, 'completion_tokens': 1948, 'total_tokens': 2020}}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "url = \"http://192.168.10.60:11434/v1/chat/completions\"\n",
    "headers = {\n",
    "    \"Authorization\": \"Bearer 289751f29159b7d25dadc86c858fa48405bba6740acb57154b00494c0308ed95\",\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "\n",
    "data = {\n",
    "    \"model\": \"gpt-oss:20b\",\n",
    "    \"messages\": [{\"role\": \"user\", \"content\": \"What is LangGraph?\"}],\n",
    "    \"temperature\": 0.1,  # è®¾ç½®è¾ƒä½çš„æ¸©åº¦\n",
    "    \"top_p\": 0.5         # é™åˆ¶é‡‡æ ·èŒƒå›´ï¼Œè¿›ä¸€æ­¥æ§åˆ¶è®¡ç®—å¤æ‚åº¦\n",
    "}\n",
    "\n",
    "response = requests.post(url, headers=headers, data=json.dumps(data))\n",
    "\n",
    "print(response.json())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "34436e94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.tools.structured.StructuredTool'>\n",
      "=**=**=**=**=**=**=**=**=**=**=**=**=**=**=**=**=**=**=**=**\n",
      "write_email\n",
      "=**=**=**=**=**=**=**=**=**=**=**=**=**=**=**=**=**=**=**=**\n",
      "æ’°å†™å¹¶å‘é€é‚®ä»¶\n",
      "=**=**=**=**=**=**=**=**=**=**=**=**=**=**=**=**=**=**=**=**\n",
      "{'to': {'title': 'To', 'type': 'string'}, 'subject': {'title': 'Subject', 'type': 'string'}, 'content': {'title': 'Content', 'type': 'string'}}\n"
     ]
    }
   ],
   "source": [
    "from langchain.tools import tool\n",
    "\n",
    "@tool\n",
    "def write_email(to: str, subject: str, content: str) -> str:\n",
    "    \"\"\"æ’°å†™å¹¶å‘é€é‚®ä»¶\"\"\"\n",
    "    return f\"é‚®ä»¶å·²å‘é€ç»™ {to}ï¼Œä¸»é¢˜ä¸º {subject}ï¼Œå†…å®¹ä¸º {content}\"\n",
    "\n",
    "print(type(write_email))\n",
    "print('=**'*20)\n",
    "print(write_email.name)\n",
    "print('=**'*20)\n",
    "print(write_email.description)\n",
    "print('=**'*20)\n",
    "print(write_email.args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab27d993",
   "metadata": {},
   "source": [
    "ä½¿ç”¨å·¥å…·\n",
    "é€šè¿‡ @tool è£…é¥°å™¨å®šä¹‰çš„å·¥å…·ï¼Œå¯ä»¥è¢«LLMä½¿ç”¨ï¼Œä½¿ç”¨çš„æ–¹æ³•æ˜¯ï¼š\n",
    "\n",
    "é€šè¿‡ bind_tools æ–¹æ³•å°†å·¥å…·ç»‘å®šåˆ°æ¨¡å‹ä¸Šï¼›\n",
    "æ¨¡å‹ä¼šæ ¹æ®ä»»åŠ¡çš„æè¿°ï¼Œå†³å®šæ˜¯å¦ä½¿ç”¨å·¥å…·ï¼Œä½¿ç”¨å“ªä¸ªå·¥å…·ï¼Œä»¥åŠå…·ä½“çš„å‚æ•°ã€‚\n",
    "å¦‚æœä½¿ç”¨å·¥å…·ï¼Œåˆ™éœ€è¦åœ¨ä»£ç ä¸­æ‰§è¡Œå·¥å…·å‡½æ•°ï¼Œå¹¶è·å–ç»“æœï¼ˆè¿™ä¸€æ­¥ä¸å¤§è¯­è¨€æ¨¡å‹æ— å…³ï¼‰ã€‚\n",
    "å¤§æ¨¡å‹çš„ä¾›åº”å•†ä»¬ï¼ˆå¦‚ OpenAIï¼‰ï¼Œè¿˜æä¾›äº†ä¸€äº›é‡è¦çš„å‚æ•°æ¥æ§åˆ¶å·¥å…·çš„ä½¿ç”¨ï¼š\n",
    "\n",
    "tool_choice: å¼ºåˆ¶é€‰æ‹©æŸä¸ªå·¥å…·æ¥æ‰§è¡Œä»»åŠ¡ï¼Œå¦‚æœä¸º any åˆ™éšæœºé€‰æ‹©ä¸€ä¸ªå·¥å…·ã€‚\n",
    "parallel_tool_calls: æ˜¯å¦ä½¿ç”¨å¤šä¸ªå·¥å…·ï¼Œå¦‚æœä¸º true åˆ™ä½¿ç”¨å¤šä¸ªå·¥å…·ï¼Œå¦‚æœä¸º False åˆ™æœ€å¤šä½¿ç”¨ä¸€ä¸ªå·¥å…·ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af11976c",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'OpenAI' object has no attribute 'bind_tools'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m model_with_tools = \u001b[43mllm\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbind_tools\u001b[49m([write_email], tool_choice=\u001b[33m\"\u001b[39m\u001b[33many\u001b[39m\u001b[33m\"\u001b[39m, parallel_tool_calls=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m      3\u001b[39m output = model_with_tools.invoke(\u001b[33m\"\u001b[39m\u001b[33må†™ä¸€å°é‚®ä»¶ç»™å¼ ä¸‰ï¼Œä¸»é¢˜æ˜¯å…³äºé¡¹ç›®è¿›å±•ï¼Œå†…å®¹æ˜¯ï¼šæˆ‘ä»¬æ­£åœ¨æŒ‰ç…§è®¡åˆ’æ¨è¿›é¡¹ç›®ï¼Œé¢„è®¡ä¸‹å‘¨å®Œæˆã€‚\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mAttributeError\u001b[39m: 'OpenAI' object has no attribute 'bind_tools'"
     ]
    }
   ],
   "source": [
    "# æ­£ç¡®çš„å·¥å…·ç»‘å®šæ–¹å¼ - ä½¿ç”¨LangChainçš„ChatOpenAI\n",
    "model_with_tools = llm.bind_tools([write_email], tool_choice=\"any\", parallel_tool_calls=False)\n",
    "\n",
    "output = model_with_tools.invoke(\"å†™ä¸€å°é‚®ä»¶ç»™å¼ ä¸‰ï¼Œä¸»é¢˜æ˜¯å…³äºé¡¹ç›®è¿›å±•ï¼Œå†…å®¹æ˜¯ï¼šæˆ‘ä»¬æ­£åœ¨æŒ‰ç…§è®¡åˆ’æ¨è¿›é¡¹ç›®ï¼Œé¢„è®¡ä¸‹å‘¨å®Œæˆã€‚\")\n",
    "\n",
    "print(\"æ¨¡å‹è¾“å‡ºç±»å‹:\", type(output))\n",
    "print(\"è¾“å‡ºå†…å®¹:\", output.content)\n",
    "print(\"å·¥å…·è°ƒç”¨:\", output.tool_calls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af6a462",
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ‰§è¡Œå·¥å…·è°ƒç”¨\n",
    "if output.tool_calls:\n",
    "    for tool_call in output.tool_calls:\n",
    "        print(f\"å·¥å…·åç§°: {tool_call['name']}\")\n",
    "        print(f\"å·¥å…·å‚æ•°: {tool_call['args']}\")\n",
    "        \n",
    "        # æ‰§è¡Œå·¥å…·\n",
    "        if tool_call['name'] == 'write_email':\n",
    "            result = write_email.invoke(tool_call['args'])\n",
    "            print(f\"å·¥å…·æ‰§è¡Œç»“æœ: {result}\")\n",
    "else:\n",
    "    print(\"æ²¡æœ‰å·¥å…·è°ƒç”¨\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65cb274b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å®Œæ•´çš„å·¥å…·è°ƒç”¨æµç¨‹ç¤ºä¾‹\n",
    "def complete_tool_calling_example():\n",
    "    \"\"\"å®Œæ•´çš„å·¥å…·è°ƒç”¨æµç¨‹ç¤ºä¾‹\"\"\"\n",
    "    \n",
    "    # 1. ç»‘å®šå·¥å…·åˆ°æ¨¡å‹\n",
    "    model_with_tools = llm.bind_tools([write_email], tool_choice=\"any\", parallel_tool_calls=False)\n",
    "    \n",
    "    # 2. è°ƒç”¨æ¨¡å‹\n",
    "    user_query = \"å¸®æˆ‘å†™ä¸€å°é‚®ä»¶ç»™æå››ï¼Œä¸»é¢˜æ˜¯ä¼šè®®é€šçŸ¥ï¼Œå†…å®¹æ˜¯æ˜å¤©ä¸‹åˆ2ç‚¹åœ¨ä¼šè®®å®¤Aå¼€ä¼šã€‚\"\n",
    "    print(f\"ç”¨æˆ·æŸ¥è¯¢: {user_query}\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    response = model_with_tools.invoke(user_query)\n",
    "    \n",
    "    # 3. æ£€æŸ¥æ˜¯å¦æœ‰å·¥å…·è°ƒç”¨\n",
    "    if response.tool_calls:\n",
    "        print(\"âœ… æ¨¡å‹å†³å®šä½¿ç”¨å·¥å…·\")\n",
    "        for tool_call in response.tool_calls:\n",
    "            print(f\"å·¥å…·åç§°: {tool_call['name']}\")\n",
    "            print(f\"å·¥å…·å‚æ•°: {tool_call['args']}\")\n",
    "            \n",
    "            # 4. æ‰§è¡Œå·¥å…·\n",
    "            if tool_call['name'] == 'write_email':\n",
    "                result = write_email.invoke(tool_call['args'])\n",
    "                print(f\"ğŸ“§ é‚®ä»¶å‘é€ç»“æœ: {result}\")\n",
    "    else:\n",
    "        print(\"âŒ æ¨¡å‹æ²¡æœ‰è°ƒç”¨å·¥å…·\")\n",
    "        print(f\"æ¨¡å‹å›å¤: {response.content}\")\n",
    "\n",
    "# è¿è¡Œç¤ºä¾‹\n",
    "complete_tool_calling_example()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-main",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
