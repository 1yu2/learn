{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c7efea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# 设置环境变量，确保不走代理\n",
    "os.environ['HTTP_PROXY'] = ''\n",
    "os.environ['HTTPS_PROXY'] = ''\n",
    "os.environ['http_proxy'] = ''\n",
    "os.environ['https_proxy'] = ''\n",
    "os.environ['NO_PROXY'] = '*'\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "model_configs = {\n",
    "    \"base_url\": \"http://192.168.10.60:11434/v1/\",\n",
    "    \"model_name\": 'gpt-oss:20b',\n",
    "    \"api_key\": \"289751f29159b7d25dadc86c858fa48405bba6740acb57154b00494c0308ed95\"\n",
    "}\n",
    "\n",
    "# 创建LangChain的ChatOpenAI客户端（支持bind_tools）\n",
    "llm = ChatOpenAI(\n",
    "    base_url=model_configs[\"base_url\"],\n",
    "    api_key=model_configs[\"api_key\"],\n",
    "    model=model_configs[\"model_name\"],\n",
    "    temperature=1.0,\n",
    "    timeout=180\n",
    ")\n",
    "\n",
    "# 测试基本调用\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"What is LangGraph?\"},\n",
    "]\n",
    "response = llm.invoke(\"What is LangGraph?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df5ae53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'openai.types.chat.chat_completion.ChatCompletion'>\n",
      "**LangGraph** is an open‑source framework for building large‑language‑model (LLM) driven applications that run across multiple turns, modalities, and external tools—all orchestrated with an explicit graph of logic.\n",
      "\n",
      "---\n",
      "\n",
      "### Why do we need a “graph” for LLM apps?\n",
      "\n",
      "| Problem | Traditional approach | LangGraph advantage |\n",
      "|---------|----------------------|---------------------|\n",
      "| **Control flow** (which tool to call next, when to stop) | Hard‑coded in the prompt or a flat chain | Explicit nodes & edges let you design branching, loops, back‑tracing, and dynamic paths |\n",
      "| **State management** (memory, context, user intent) | Implicitly baked into the prompt | `state` is a first‑class dictionary that can be read/written by any node |\n",
      "| **Tool integration** (search, API calls, spreadsheet, etc.) | Often a generic “tool‑calling” prompt that may fail | Dedicated nodes for APIs, DBs, HTTP, even custom Python functions |\n",
      "| **Debugging / monitoring** | You have to trace logs manually | Built‑in visualizer, event hooks, live tracing |\n",
      "| **Extensibility** | Each app needs its own plumbing | Graph definition is reusable; you can import/export graphs |\n",
      "\n",
      "---\n",
      "\n",
      "## Core Concepts\n",
      "\n",
      "| Concept | What it is | How it helps |\n",
      "|---------|------------|--------------|\n",
      "| **Graph** | A directed graph where each vertex is a *node* and edges define the allowed next steps. | Lets you program logic as flowcharts instead of linear code. |\n",
      "| **Node** | A Python callable (function, LLM chain, tool…) that receives the current `state` and emits a new `state`. | Encapsulates a unit of work (e.g. “ask LLM”, “call API”, “summarize”). |\n",
      "| **State** | A dictionary that persists across nodes. Keys can be anything: text, JSON, scores, context vectors. | Gives you full control over what data travels through the graph. |\n",
      "| **Edge** | The connection from one node to another. Usually you’ll annotate it with a *condition* (`if`, `else`, or predicates). | Directs the flow based on runtime data. |\n",
      "| **Memory** | External store (vector DB, relational DB, Redis…) that nodes can read/write. | Enables retrieval‑augmented generation, conversation history, knowledge bases. |\n",
      "| **Event Hooks** | Callbacks fired before/after nodes run or on errors. | Logging, monitoring, A/B testing. |\n",
      "| **Visualizer** | Web UI that shows the graph, the current state, and the runtime path. | Debugging made painless. |\n",
      "| **Tool Nodes** | Nodes that wrap an LLM tool call (HTTP, Python function, SQL). | Clean separation; you can swap tools without changing the graph structure. |\n",
      "| **ReAct / Memory‑Augmented Patterns** | Standard patterns like “ReAct” (Reason + Act), “Tool‑chain” are available as pre‑built node templates. | Rapid prototyping. |\n",
      "\n",
      "---\n",
      "\n",
      "## How a LangGraph App Works\n",
      "\n",
      "1. **Define the Graph**  \n",
      "   ```python\n",
      "   from langgraph import Graph\n",
      "   g = Graph()\n",
      "   g.add_nodes({\n",
      "       \"read_user\": read_user_node,\n",
      "       \"extract_query\": query_node,\n",
      "       \"search_db\": search_node,\n",
      "       \"llm_answer\": answer_node,\n",
      "       \"finalize\": finalize_node,\n",
      "   })\n",
      "   g.set_start(\"read_user\")\n",
      "   g.set_end(\"finalize\")\n",
      "   # Add edges\n",
      "   g.add_edge(\"read_user\",\"extract_query\")\n",
      "   g.add_edge(\"extract_query\",\"search_db\")\n",
      "   g.add_edge(\"search_db\",\"llm_answer\")\n",
      "   g.add_edge(\"llm_answer\",\"finalize\")\n",
      "   ```\n",
      "2. **Run the Graph**  \n",
      "   ```python\n",
      "   result = g.run(initial_state={\"message\": \"Show me the latest quarterly earnings\"} )\n",
      "   ```\n",
      "3. **Graph handles**  \n",
      "   * Passes `state` along nodes automatically.  \n",
      "   * Evaluates any conditional edges.  \n",
      "   * Traces the path for debugging.  \n",
      "   * Stores intermediate results in memory if configured.\n",
      "\n",
      "---\n",
      "\n",
      "## When to Use LangGraph\n",
      "\n",
      "| Scenario | Example Use‑case |\n",
      "|-----------|-----------------|\n",
      "| **Multi‑turn dialogue** | Chatbot that keeps context, updates intent, and calls external APIs on demand. |\n",
      "| **Adaptive reasoning** | Decision support system that loops between inference steps until a confidence threshold is reached. |\n",
      "| **Tool‑heavy pipelines** | A data‑augmentation pipeline that calls language models, then a QA tool, then stores results in a vector DB. |\n",
      "| **Explainable AI** | A graph that logs every reasoning step, so the final answer can be traced back to specific prompts or external data. |\n",
      "| **Rapid prototyping** | Swap LLM backends (OpenAI, Anthropic, local) without touching the graph. |\n",
      "\n",
      "---\n",
      "\n",
      "## Ecosystem & Integrations\n",
      "\n",
      "| Tool | Where it fits in LangGraph |\n",
      "|------|----------------------------|\n",
      "| **LangChain** | You can import any LangChain chain or prompt template as a node (`chain_node`). |\n",
      "| **LangSmith** | Built‑in monitoring & tracing (metrics, logs, vector embeddings). |\n",
      "| **LangGraph UI** | Live graph viewer, replay history, error inspection. |\n",
      "| **Tools**: `requests`, `python` functions, `SQL`, `Google Search`, `OpenAI`, `Anthropic`, `OpenRouter`, `Azure`, `OpenAI Embedding APIs`. |\n",
      "| **Memory**: FAISS, Pinecone, Weaviate, Chroma, Qdrant, Supabase. |\n",
      "| **Embeddings**: OpenAI, Cohere, HuggingFace, all accessible as node wrappers. |\n",
      "\n",
      "---\n",
      "\n",
      "## Example: Simple QA Graph\n",
      "\n",
      "```python\n",
      "from langgraph import Graph\n",
      "from langgraph.operators import ToolNode, LLMNode, State\n",
      "\n",
      "def search_tool(query: str) -> str:\n",
      "    # pretend we hit a vector DB\n",
      "    return \"Retrieved content about quarterly earnings.\"\n",
      "\n",
      "g = Graph()\n",
      "g.add_nodes({\n",
      "    \"extract_query\": LLMNode(\"extract_query_prompt\"),\n",
      "    \"search\": ToolNode(\"search\", function=search_tool),\n",
      "    \"respond\": LLMNode(\"answer_prompt\")\n",
      "})\n",
      "g.set_start(\"extract_query\")\n",
      "g.set_end(\"respond\")\n",
      "g.add_edge(\"extract_query\", \"search\")\n",
      "g.add_edge(\"search\", \"respond\")\n",
      "\n",
      "response = g.run({\"user_input\": \"What did the company earn this quarter?\"})\n",
      "print(response[\"final_answer\"])\n",
      "```\n",
      "\n",
      "Output (mocked):\n",
      "\n",
      "```\n",
      "The company earned $3.2B in the latest quarter, representing a 12% YoY increase.\n",
      "```\n",
      "\n",
      "---\n",
      "\n",
      "## Getting Started\n",
      "\n",
      "1. **Install**  \n",
      "   ```bash\n",
      "   pip install langgraph\n",
      "   ```\n",
      "2. **Read the docs** – https://docs.langgraph.xyz  \n",
      "3. **Try the playground** – Build a graph in a Jupyter notebook or Google Colab.  \n",
      "4. **Deploy** – Export your graph as a serverless function, wrap with FastAPI, or use it in a custom UI.\n",
      "\n",
      "---\n",
      "\n",
      "### Takeaway\n",
      "\n",
      "LangGraph turns the black‑box chain‑of‑thought model into a *graph* you can program, inspect, and evolve. It’s the glue that lets you:\n",
      "\n",
      "- Keep state safe and explicit,\n",
      "- Call any external tool cleanly,\n",
      "- Trace every decision step,\n",
      "- Swap LLMs without touching logic.\n",
      "\n",
      "If you’re building an AI app that needs to reason over past context, talk to APIs, and deliver explainable answers, LangGraph gives you the right abstraction to avoid reinventing plumbing every time.\n"
     ]
    }
   ],
   "source": [
    "print(type(response))\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c717d33a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 'chatcmpl-464', 'object': 'chat.completion', 'created': 1754559472, 'model': 'gpt-oss:20b', 'system_fingerprint': 'fp_ollama', 'choices': [{'index': 0, 'message': {'role': 'assistant', 'content': '## LangGraph – A Graph‑Based Framework for LLM‑Powered Apps\\n\\n**LangGraph** is an open‑source framework (part of the LangChain ecosystem) that lets you build, orchestrate, and debug complex, stateful workflows powered by large language models (LLMs).  \\nInstead of writing a monolithic “agent” or a linear script, you describe a *graph* of nodes (functions, prompts, tools, etc.) and the edges that connect them. The framework takes care of:\\n\\n| What it does | Why it matters |\\n|--------------|----------------|\\n| **Graph execution** | You can run nodes in sequence, in parallel, or conditionally based on the current state. |\\n| **Stateful reasoning** | Each node receives the full “state” (conversation history, memory, tool outputs, etc.) and can update it. |\\n| **Tool integration** | Built‑in connectors for APIs, databases, custom functions, and the LangChain tool‑calling system. |\\n| **LLM orchestration** | Wrap any LLM (OpenAI, Anthropic, Gemini, local models, etc.) as a node. |\\n| **Memory & context** | Persist state across turns, enabling long‑term memory or multi‑step reasoning. |\\n| **Debugging & visualization** | LangGraph Studio lets you see the graph, step through execution, and inspect state at each node. |\\n| **Composable** | Reuse sub‑graphs (e.g., a “search‑then‑summarize” sub‑graph) across multiple applications. |\\n\\n---\\n\\n### How It Looks in Code\\n\\n```python\\nfrom langgraph.graph import StateGraph, MessagesState\\nfrom langchain_openai import ChatOpenAI\\nfrom langchain.tools import DuckDuckGoSearchRun\\n\\n# 1. Define the LLM node\\nllm = ChatOpenAI(model=\"gpt-4o-mini\")\\n\\ndef ask_llm(state: MessagesState) -> MessagesState:\\n    # Pass the conversation history to the LLM\\n    response = llm.invoke(state[\"messages\"])\\n    state[\"messages\"].append(response)\\n    return state\\n\\n# 2. Define a tool node\\nsearch_tool = DuckDuckGoSearchRun()\\n\\ndef search(state: MessagesState) -> MessagesState:\\n    query = state[\"messages\"][-1][\"content\"]  # last user message\\n    result = search_tool.run(query)\\n    state[\"search_result\"] = result\\n    return state\\n\\n# 3. Build the graph\\ngraph = StateGraph(MessagesState)\\ngraph.add_node(\"ask_llm\", ask_llm)\\ngraph.add_node(\"search\", search)\\n\\n# Connect nodes: after asking the LLM, decide whether to search\\ngraph.add_conditional_edges(\\n    \"ask_llm\",\\n    lambda state: \"search\" if \"search\" in state[\"messages\"][-1][\"content\"].lower() else \"ask_llm\",\\n    {\"search\": \"search\", \"ask_llm\": \"ask_llm\"}\\n)\\n\\n# Final node that returns the answer\\ndef finish(state: MessagesState) -> MessagesState:\\n    # Combine LLM answer with search result if available\\n    if \"search_result\" in state:\\n        state[\"messages\"].append({\"role\": \"assistant\", \"content\": state[\"search_result\"]})\\n    return state\\n\\ngraph.add_node(\"finish\", finish)\\ngraph.set_entry_point(\"ask_llm\")\\ngraph.set_finish_point(\"finish\")\\n\\n# 4. Run the graph\\napp = graph.compile()\\nresponse = app.invoke({\"messages\": [{\"role\": \"user\", \"content\": \"Tell me about the Eiffel Tower\"}]})\\nprint(response[\"messages\"][-1][\"content\"])\\n```\\n\\n*What happens?*  \\n1. The user message is sent to the LLM.  \\n2. The LLM’s reply is examined; if it contains the word “search”, the graph routes to the `search` node.  \\n3. The search result is stored in state and the graph returns to `finish`, which merges the search output with the LLM answer.  \\n\\n---\\n\\n### Key Concepts\\n\\n| Concept | Description |\\n|---------|-------------|\\n| **State** | A dictionary (or custom class) that carries all data through the graph. |\\n| **Node** | A callable that receives the state, performs work (LLM call, tool usage, logic), and returns an updated state. |\\n| **Edge** | Defines the flow between nodes. Can be static or conditional based on the state. |\\n| **Graph** | The collection of nodes and edges. You compile it into an executable app. |\\n| **LangGraph Studio** | A web UI that visualizes the graph, shows node execution order, and lets you step through runs. |\\n\\n---\\n\\n### Why Use LangGraph?\\n\\n| Scenario | LangGraph Advantage |\\n|----------|---------------------|\\n| **Multi‑step reasoning** | Each step can be a separate node, making the logic clear and testable. |\\n| **Tool‑calling** | Wrap any external API or function as a node; the graph handles when to call it. |\\n| **Long‑term memory** | Persist state across turns; you can store embeddings, search results, or user preferences. |\\n| **Debugging** | Visualize the flow, inspect state at each node, and replay runs. |\\n| **Modularity** | Build reusable sub‑graphs (e.g., “summarize”, “translate”) and compose them into larger workflows. |\\n\\n---\\n\\n### Getting Started\\n\\n```bash\\npip install langgraph langchain-openai duckduckgo-search\\n```\\n\\n```python\\n# Quick demo\\nfrom langgraph.graph import StateGraph, MessagesState\\nfrom langchain_openai import ChatOpenAI\\n\\nllm = ChatOpenAI(model=\"gpt-4o-mini\")\\n\\ndef ask(state: MessagesState) -> MessagesState:\\n    response = llm.invoke(state[\"messages\"])\\n    state[\"messages\"].append(response)\\n    return state\\n\\ngraph = StateGraph(MessagesState)\\ngraph.add_node(\"ask\", ask)\\ngraph.set_entry_point(\"ask\")\\ngraph.set_finish_point(\"ask\")\\napp = graph.compile()\\n\\nprint(app.invoke({\"messages\": [{\"role\": \"user\", \"content\": \"Hello\"}]}}))\\n```\\n\\n---\\n\\n### Where to Learn More\\n\\n| Resource | What you’ll find |\\n|----------|------------------|\\n| **GitHub repo** | `https://github.com/langchain-ai/langgraph` – source code, docs, examples. |\\n| **Docs** | `https://langgraph.org/docs` – tutorials, API reference, best practices. |\\n| **LangGraph Studio** | `https://studio.langgraph.org` – visual debugging (requires a free account). |\\n| **Community** | LangChain Discord, StackOverflow tags, and the LangChain forum. |\\n\\n---\\n\\n### Bottom Line\\n\\nLangGraph turns the often‑messy process of building LLM‑driven applications into a **structured, visual, and testable** workflow. By treating each step as a node in a graph, you gain fine‑grained control over when and how the LLM, tools, and memory interact—making it easier to build reliable, maintainable, and explainable AI systems.', 'reasoning': 'The user asks: \"What is LangGraph?\" They likely refer to a library or framework. LangGraph is a framework for building LLM-powered applications, especially for building graph-based workflows. It\\'s by the LangChain team? Actually LangGraph is a framework by the LangChain team for building LLM-powered applications with graph-based logic. It allows building stateful, multi-step reasoning, and can be used for building agents, workflows, etc. It uses graph nodes, edges, and state. It can integrate with LLMs, memory, tools, etc. It is open-source. It is part of the LangChain ecosystem. It uses the concept of \"LangGraph\" to orchestrate LLM calls, memory, and tool usage. It can be used to build complex applications like chatbots, data pipelines, etc. It uses a graph of nodes that can be executed sequentially or conditionally. It can be used with LangChain\\'s LLMs, memory, and tools. It also supports \"LangGraph Studio\" for visualizing and debugging. It can be used with Python. It can be integrated with LangChain\\'s \"Agent\" concept. It can also be used with \"LangGraph\" to build \"LangGraph\" flows. It can be used to build \"LangGraph\" for \"LangGraph\" tasks. The answer should explain what LangGraph is, its purpose, key features, how it works, and maybe mention its relation to LangChain. Also mention that it\\'s open-source, available on GitHub, and can be installed via pip. Provide a brief example. Also mention that it\\'s used for building LLM-powered applications with graph-based logic, enabling multi-step reasoning, tool usage, memory, etc. Also mention that it can be used for building agents, workflows, and can be integrated with other frameworks. Provide a concise but thorough explanation. Also mention that it\\'s part of the LangChain ecosystem. Provide a short code snippet. Also mention that it uses \"state\" and \"nodes\" and \"edges\". Also mention that it can be used with \"LangGraph Studio\" for debugging. Also mention that it\\'s built on top of \"LangChain\" and \"LangChain Core\". Also mention that it\\'s used for building \"LangGraph\" for \"LangGraph\" tasks. Provide a final answer.'}, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 72, 'completion_tokens': 1948, 'total_tokens': 2020}}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "url = \"http://192.168.10.60:11434/v1/chat/completions\"\n",
    "headers = {\n",
    "    \"Authorization\": \"Bearer 289751f29159b7d25dadc86c858fa48405bba6740acb57154b00494c0308ed95\",\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "\n",
    "data = {\n",
    "    \"model\": \"gpt-oss:20b\",\n",
    "    \"messages\": [{\"role\": \"user\", \"content\": \"What is LangGraph?\"}],\n",
    "    \"temperature\": 0.1,  # 设置较低的温度\n",
    "    \"top_p\": 0.5         # 限制采样范围，进一步控制计算复杂度\n",
    "}\n",
    "\n",
    "response = requests.post(url, headers=headers, data=json.dumps(data))\n",
    "\n",
    "print(response.json())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "34436e94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.tools.structured.StructuredTool'>\n",
      "=**=**=**=**=**=**=**=**=**=**=**=**=**=**=**=**=**=**=**=**\n",
      "write_email\n",
      "=**=**=**=**=**=**=**=**=**=**=**=**=**=**=**=**=**=**=**=**\n",
      "撰写并发送邮件\n",
      "=**=**=**=**=**=**=**=**=**=**=**=**=**=**=**=**=**=**=**=**\n",
      "{'to': {'title': 'To', 'type': 'string'}, 'subject': {'title': 'Subject', 'type': 'string'}, 'content': {'title': 'Content', 'type': 'string'}}\n"
     ]
    }
   ],
   "source": [
    "from langchain.tools import tool\n",
    "\n",
    "@tool\n",
    "def write_email(to: str, subject: str, content: str) -> str:\n",
    "    \"\"\"撰写并发送邮件\"\"\"\n",
    "    return f\"邮件已发送给 {to}，主题为 {subject}，内容为 {content}\"\n",
    "\n",
    "print(type(write_email))\n",
    "print('=**'*20)\n",
    "print(write_email.name)\n",
    "print('=**'*20)\n",
    "print(write_email.description)\n",
    "print('=**'*20)\n",
    "print(write_email.args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab27d993",
   "metadata": {},
   "source": [
    "使用工具\n",
    "通过 @tool 装饰器定义的工具，可以被LLM使用，使用的方法是：\n",
    "\n",
    "通过 bind_tools 方法将工具绑定到模型上；\n",
    "模型会根据任务的描述，决定是否使用工具，使用哪个工具，以及具体的参数。\n",
    "如果使用工具，则需要在代码中执行工具函数，并获取结果（这一步与大语言模型无关）。\n",
    "大模型的供应商们（如 OpenAI），还提供了一些重要的参数来控制工具的使用：\n",
    "\n",
    "tool_choice: 强制选择某个工具来执行任务，如果为 any 则随机选择一个工具。\n",
    "parallel_tool_calls: 是否使用多个工具，如果为 true 则使用多个工具，如果为 False 则最多使用一个工具。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af11976c",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'OpenAI' object has no attribute 'bind_tools'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m model_with_tools = \u001b[43mllm\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbind_tools\u001b[49m([write_email], tool_choice=\u001b[33m\"\u001b[39m\u001b[33many\u001b[39m\u001b[33m\"\u001b[39m, parallel_tool_calls=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m      3\u001b[39m output = model_with_tools.invoke(\u001b[33m\"\u001b[39m\u001b[33m写一封邮件给张三，主题是关于项目进展，内容是：我们正在按照计划推进项目，预计下周完成。\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mAttributeError\u001b[39m: 'OpenAI' object has no attribute 'bind_tools'"
     ]
    }
   ],
   "source": [
    "# 正确的工具绑定方式 - 使用LangChain的ChatOpenAI\n",
    "model_with_tools = llm.bind_tools([write_email], tool_choice=\"any\", parallel_tool_calls=False)\n",
    "\n",
    "output = model_with_tools.invoke(\"写一封邮件给张三，主题是关于项目进展，内容是：我们正在按照计划推进项目，预计下周完成。\")\n",
    "\n",
    "print(\"模型输出类型:\", type(output))\n",
    "print(\"输出内容:\", output.content)\n",
    "print(\"工具调用:\", output.tool_calls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af6a462",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 执行工具调用\n",
    "if output.tool_calls:\n",
    "    for tool_call in output.tool_calls:\n",
    "        print(f\"工具名称: {tool_call['name']}\")\n",
    "        print(f\"工具参数: {tool_call['args']}\")\n",
    "        \n",
    "        # 执行工具\n",
    "        if tool_call['name'] == 'write_email':\n",
    "            result = write_email.invoke(tool_call['args'])\n",
    "            print(f\"工具执行结果: {result}\")\n",
    "else:\n",
    "    print(\"没有工具调用\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65cb274b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 完整的工具调用流程示例\n",
    "def complete_tool_calling_example():\n",
    "    \"\"\"完整的工具调用流程示例\"\"\"\n",
    "    \n",
    "    # 1. 绑定工具到模型\n",
    "    model_with_tools = llm.bind_tools([write_email], tool_choice=\"any\", parallel_tool_calls=False)\n",
    "    \n",
    "    # 2. 调用模型\n",
    "    user_query = \"帮我写一封邮件给李四，主题是会议通知，内容是明天下午2点在会议室A开会。\"\n",
    "    print(f\"用户查询: {user_query}\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    response = model_with_tools.invoke(user_query)\n",
    "    \n",
    "    # 3. 检查是否有工具调用\n",
    "    if response.tool_calls:\n",
    "        print(\"✅ 模型决定使用工具\")\n",
    "        for tool_call in response.tool_calls:\n",
    "            print(f\"工具名称: {tool_call['name']}\")\n",
    "            print(f\"工具参数: {tool_call['args']}\")\n",
    "            \n",
    "            # 4. 执行工具\n",
    "            if tool_call['name'] == 'write_email':\n",
    "                result = write_email.invoke(tool_call['args'])\n",
    "                print(f\"📧 邮件发送结果: {result}\")\n",
    "    else:\n",
    "        print(\"❌ 模型没有调用工具\")\n",
    "        print(f\"模型回复: {response.content}\")\n",
    "\n",
    "# 运行示例\n",
    "complete_tool_calling_example()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-main",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
